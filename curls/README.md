## IMPORTANT!!!

This directory contains the necessary headers and cookies data that are necessary to run the scraper which makes the requests using the corresponding headers and cookies for each API endpoint. As these data are sensitive, I have excluded them from commiting to git. Please collect yours from the network tab of your browser's developer tool. Steps to follow:

1. Click the `Copy as cURL (Bash)` option in the API that will be used for each scraper in this repository.

2. Run a demo by pasting the copied text in a bash terminal (Windows users can run it on their Git Bash) and see if you get the desired output.

3. Create a file naming it according to the corresponding spider's name.

4. Paste the copied cURL script into the newly created file.
